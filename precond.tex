% Prelim, Chapter 4
% by Rachel Slaybaugh

\chapter{Preconditioning}
\label{sec:Chp4}
The new ``grand challenge'' problems facing the nuclear transport community are large and complex. Cutting edge methods are required to solve them. While the second and third chapters discussed new methods that enable the solution of such problems, low-cost preconditioners that can reduce the number of iterations needed for convergence will be invaluable. In Benzi et.\ al.'s 2002 survey paper on preconditioning techniques for large linear systems they state ``it is widely recognized that preconditioning is the most critical ingredient in the development of efficient solvers for challenging problems in scientific computation \cite{Benzi2002}.'' 

This is true for Krylov methods in particular because the memory required and cost per iteration increase dramatically with the number of iterations \cite{Benzi2002}. And, as discussed in Chapter \ref{sec:Chp3}, Krylov methods can converge very slowly for poorly conditioned systems. When this happens the eigenvector is not converged in a reasonable number of iterations and RQI cannot converge the eigenvalue. Preconditioning is therefore required to make RQI useful. 

This chapter is about the new preconditioner added to Denovo. First some background information that includes an introduction to preconditioning, an overview of preconditioners used in the nuclear community, and a discussion of multigrid methods is given. Next, past and related work is discussed. Finally the new preconditioner is explained next, and results demonstrating its impact are given. 

%-------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------
\section{Background}
The general idea of preconditioning is to transform the system of interest into another equivalent system that has more favorable properties, for example one with a smaller condition number. A preconditioner is a matrix that induces such a transformation by improving the spectral properties of the problem being solved. Let $\ve{G}$ be a non-singular preconditioner, then $\ve{A}x=b$ can be transformed in the following ways \cite{Benzi2002}: 
%
\begin{alignat}{3}
  \ve{G}^{-1}\ve{A}x &= \ve{G}^{-1}b  &  &\text{left preconditioning,} \\
  \ve{AG}^{-1}y &= b, \qquad  &x = \ve{G}^{-1}y \qquad &\text{right preconditioning, and } \\
  \ve{G}_{1}^{-1}\ve{AG}_{2}^{-1}y &= \ve{G}_{1}^{-1}b, \qquad \ve{G} = \ve{G}_{1}\ve{G}_{2}, \qquad  &x = \ve{G}_{2}^{-1}y  \qquad &\text{split preconditioning.} 
\end{alignat}

If $\ve{A}$ and/or $\ve{G}$ are non-normal then each of the preconditioning constructs will likely give different behavior, though they will converge to the same answer because the matrices are similar and therefore have the same eigenvalues  \cite{Benzi2002}. Right preconditioning leaves the right hand side of the equation unaffected and does not change the norm of the residual, which is used for convergence testing in most iterative methods. Right preconditioning is usually preferred over left preconditioning for iterative solvers for this reason \cite{Knoll2004}. A right preconditioner was implemented in this work, and the remaining discussion will be presented in right preconditioner format. 

There are two extremes between which all other preconditioners lie: $\ve{G} = \ve{A}$, in which case the solution can be found directly, and $\ve{G} = \ve{I}$, which will have no effect.  In general, the matrix $\ve{A}\ve{G}^{-1}$ is never formed. The preconditioner can be applied by using some method to solve $\ve{G}y=c \to y \approx \ve{G}^{-1}c$, or by otherwise implementing the action of $\ve{G}^{-1}$ without ever explicitly forming and inverting $\ve{G}$ \cite{Benzi2002}, \cite{Trefethen1997}. 

Functionally, a good preconditioner should make the system easier to solve and result in faster convergence. It should also be cheap to construct and apply. These tend to be competing goals in that the easier a preconditioner is to construct and apply the less it typically does to improve convergence. Generally, a preconditioner is good if $\ve{A}\ve{G}^{-1}$ is not too far from normal and its eigenvalues are clustered \cite{Trefethen1997}. 

There are many different types of preconditioners, but they can be put into two general categories: matrix-based and physics-based. Matrix-based preconditioners rely entirely on the structure of the matrix $\ve{A}$, regardless of the physics going on in the problem. That is, these methods do not change when the underlying problem changes. This can be a very useful property because matrix-based methods are then broadly applicable and do not require any understanding of the physical problem. Extrapolation methods and incomplete factorization are examples of a matrix-based preconditioners \cite{Trefethen1997}.

Physics-based preconditioning uses knowledge about the physics of the problem in question to guide the creation of the preconditioner. This means that some methods only work with certain kinds of problems, and that the preconditioners may have to be tailored or adapted for different applications. However, such methods take advantage of knowing something about the problem and can be more effective than matrix-based methods for the range of problems for which they are intended. Rebalance and synthetic acceleration are examples of physics-based preconditioners \cite{Trefethen1997}.

%-----------------------------------------------------------------------------------------------
\subsection{Preconditioners in the Nuclear Community}
A variety of acceleration methods have been used by the nuclear computational community over the years. This subsection gives a high-level overview of some common methods. In 2002 Adams and Larsen put together a comprehensive overview of the development of iterative methods for solving the \Sn transport equation \cite{Adams2002}. For more detail and history about each method, refer to this publication. 

\subsubsection{Extrapolation Methods}
Extrapolation methods were historically used to accelerate $k$-eigenvalue calculations. These tend to be based on simple iterative solvers or polynomial approximations. Using one step of a simple iterative method such as Jacobi, Gauss Seidel, or successive over relaxation (SOR) can be used at the outset of a problem to serve as a preconditioner \cite{Trefethen1997}. When applied to neutron transport, an overrelaxation method looks like $\ve{F}_{i+1} = \omega(\tilde{\ve{F}}_i - \ve{F}_i) + \ve{F}_i$ with $1 \le \omega < 2$, where the unaccelerated new iterate for the fission source is designated $\tilde{\ve{F}}_i$ \cite{Lewis1993}.

Polynomial preconditioners create a matrix polynomial $\ve{G}^{-1} = p(\ve{A})$, where $p(\ve{A})$ serves as a polynomial approximation to $\ve{A}^{-1}$. Common polynomial choices are truncated Neumann series and Chebyshev polynomials. An advanced variation of this method is to determine the polynomial coefficients adaptively \cite{Trefethen1997}. When Chebyshev polynomials are applied to the transport equation, $\ve{F}_{i+1} = \ve{F}_i + \alpha_{i+1}(\tilde{\ve{F}}_i - \ve{F}_i) + \beta_i(\ve{F}_i - \ve{F}_{i-1})$, where $\alpha$ and $\beta$ are iteration dependent \cite{Lewis1993}. Neither of these extrapolation methods are widely used today as they have not been very successful in multi-dimensional problems. There are analogous versions of these methods for fixed source problems as well \cite{Alcouffe1977}.  

\subsubsection{Rebalance}
Rebalance methods are designed to accelerate iteration on the scattering or fission source by imposing a balance condition on the unconverged solution over either coarse or fine regions. If the region is coarse, this is a coarse-grid approximation preconditioner since fine-grid physics are excluded. If the region is fine, this is a local approximation preconditioner where short range effects are permitted and long range interactions are excluded \cite{Trefethen1997}, \cite{Adams2002}.

In either case, the unaccelerated solution is multiplied by a constant in each region that satisfies the balance equation. The rebalance process adjusts the average amplitude of the flux over a region and the iteration adjusts the space-angle distribution. The notion is that these processes work in concert to eliminate all error modes simultaneously \cite{Adams2002}. 

Coarse mesh rebalance (CMR) has been more successful than the fine version and is therefore used more often. In practice, choosing regions properly must be done with care \cite{Lewis1993}. Traditional forms of CMR can be unstable for problems where the spatial mesh is large compared to the neutron mean-free-path and where the scattering ratio is close to unity \cite{Alcouffe1977}. 

\subsubsection{Incomplete Factorizations}
Incomplete factorization methods were first introduced in the 1950s by Buleev, and independently by Varga. In the late 1970s these methods started gaining popularity, and since then have been under active development. The idea is to partially factor the matrix $\ve{A}$ such that the resulting matrices are sparse enough to take advantage of sparse solver methods, but close enough to $\ve{A}$ to be valuable as preconditioners \cite{Benzi2002}.

Incomplete Cholesky (IC) or incomplete LU (ILU) factorization have both been extensively used within the nuclear community. The Cholesky version is used when $\ve{A}$ is symmetric and LU when non-symmetric. Factorizations typically destroy sparsity, resulting in matrices that are denser than the originals and making their use more costly. Different variations of incomplete factorization methods address this by permitting the new matrices to have values only in positions where $\ve{A}$ has values, allowing a prescribed amount of fill in, or only saving entries greater than some tolerance \cite{Trefethen1997}, \cite{Patton2002}, \cite{Oliveira1998}.

\subsubsection{Synthetic Acceleration}
In synthetic acceleration a low-order approximation to the transport operator is used to accelerate the full transport problem \cite{Lewis1993}. If a system is discretized with a high-order method it can be preconditioned with a lower-order approximation. The low-order method is often much sparser than the original, but still captures the general behavior of the problem \cite{Trefethen1997}. This is generally done with either the diffusion equation, giving diffusion synthetic acceleration (DSA), or a transport equation  that is simpler than the one actually being solved, giving transport synthetic acceleration (TSA). These can be used for fission or fixed source problems \cite{Adams2002}. 

%Synthetic methods have at least two iteration stages. The simplest way to illustrate this is to use source iteration for the within-group solver. The first step is a transport sweep, giving:
%%
%\begin{equation}
%  \ve{L}\psi^{(l+\frac{1}{2})} = \ve{S}\psi^{(l)} + q \:, \qquad l \ge 0 \:.
%  \label{eq:synthetic}
%\end{equation}
%%
%Here $l$ is the iteration index, $\ve{L}$ is the transport operator, $\ve{S}$ is the scattering matrix, and $q$ is the external source. Equation \eqref{eq:synthetic} is subtracted from the exact equation to get an expression for an exact additive correction:
%%
%\begin{equation}
%  \bigl( \ve{L} - \ve{S} \bigr)\bigl( \psi - \psi^{(l+\frac{1}{2})} \bigr) = \ve{S}\bigl(\psi^{(l+\frac{1}{2})} - \psi^{(l)} \bigr) \:.
%  \label{eq:correction}
%\end{equation}
%%
%This cannot be solved exactly, but the idea of synthetic methods is to find an $\ve{G}^{-1} \approx (\ve{L} - \ve{S})^{-1}$ that will be easier to evaluate than $(\ve{L} - \ve{S})^{-1}$. Equation \eqref{eq:synthetic} is solved by a high-order scheme and Equation \eqref{eq:correction} is solved with a low-order approximation. If the synthetic system converges, it must satisfy the original transport equation \cite{Adams2002}. 

Original DSA formulations exhibited instability in some instances. In 1977 Alcouffe demonstrated with diamond difference that stability can be obtained by using a consistent spatial differencing scheme \cite{Alcouffe1977}. The transport and diffusion equations cannot be discretized independently or the diffusion equation may not satisfy the original transport equation. The discretization of the diffusion equation must therefore be derived from the discretization of the transport equation such that they are consistent. Alcouffe's idea has since been extended to accelerate currents, to more dimensions, and to different spatial discretizations \cite{Larsen1982}. 

DSA can be implemented a few different ways, each of which performs a transport sweep and then solves a corrected diffusion equation. Different terms in the diffusion equation can be corrected for different problem types \cite{Alcouffe1977}. DSA has been found to degrade in the presence of material discontinuities in addition to when it is not consistently derived. However, Warsa et.\ al.\ found that when DSA is used as a preconditioner for Krylov iterative methods, it is effective even when only partially consistent \cite{Warsa2004}.

To address the concerns about spatial discretization associated with DSA, a low-order transport solve could be used to find the correction term instead. Larsen and Miller developed a method that neglects scattering in the low-order transport equation, but this can cause instability in multi-dimensional problems. Ramone et.\ al.\ proposed including some scattering by using a tunable parameter. A high-order transport equation is solved first, then a transport equation with a coarse quadrature and reduced scattering is solved to find the correction that is applied to the new iterate of the scalar flux \cite{Ramone1997}. This algorithm can be viewed as Richardson iteration with a coarse operator preconditioner. Synthetic acceleration methods are widely used and actively researched in the transport community today.

%-----------------------------------------------------------------------------------------------------
\subsection{Multigrid Methods}
The new preconditioner added to Denovo does multigrid in the energy dimension. To understand why multigrid in energy makes sense, why multigrid methods work must be understood. The material presented here is largely from \emph{A Multigrid Tutorial} by Briggs, Henson, and McCormick \cite{Briggs2000}, supplemented by Dr. Strang's lectures on multigrid and preconditioners available as MIT Open Courseware \cite{Strang}.

In what follows, the system of equations will be denoted as $\ve{A}u=b$ where $u$ will represent the exact solution and $v$ will indicate the approximate solution.  A certain grid will be denoted as $\Omega^{h}$ where $h$ is the grid spacing, and vectors on that grid are $u^{h}$. The $i$th entry in a vector is $u_{i}$. The algebraic error is given by $e = u - v$. Since $e$ cannot be calculated exactly (otherwise the answer would be known), the residual, $r = b - \ve{A}v$ is often used. 

The error in any guess can be written as a combination of Fourier modes. A Fourier mode is described by a wavenumber, $k$, which denotes the frequency of oscillation. The $j$th Fourier mode is 
% 
\begin{equation}
    e_{j} = \sin\bigl(\frac{jk\pi}{n}\bigr) \:, \qquad 0 \le j \le n \:, \qquad 1 \le k \le n-1 \:.
\end{equation} 
%
There are $k$ half sine waves that comprise $e$ on the domain of the problem. The term $e_{k}$ indicates an $e$ with wavenumber $k$. The $k$th wave has $\frac{k}{2}$ full sine waves with a wavelength of $\frac{2}{k}$. Modes with wavenumbers in the range $1 \le k \le \frac{n}{2}$ are called low-frequency or smooth modes and those in $\frac{n}{2} \le k \le n-1$ are called high-frequency or oscillatory modes. Figure \ref{fig:FourierModes} shows what a few different modes look like. 
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.45\textwidth, height=0.2\textheight] {FourierModes}
   \end{center}
   \caption{Three Fourier Modes \cite{Briggs2000}}
   \label{fig:FourierModes}
\end{figure}

Iterative methods, also referred to as smoothers or relaxers, remove high frequency error components very quickly, but take a long time to remove the low frequency components. This is illustrated in Figure~\ref{fig:FourierError}, where weighted Jacobi was applied to Fourier modes of different frequencies on an $n$ = 64 grid. The error is reduced much faster for higher modes. 
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.5\textwidth, height=0.3\textheight] {FourierError}
   \end{center}
   \caption{Log of Error As a Function of Iteration Count When a Relaxer is Applied to Three Fourier Modes \cite{Briggs2000}}
   \label{fig:FourierError}
\end{figure}
%
The rapid removal of oscillatory and slow removal of coarse error modes is often seen in practice. Figure~\ref{fig:MGerrorExample} shows an error plot that exhibits this behavior
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.7\textwidth, height=0.4\textheight] {MGerrorExample}
   \end{center}
   \caption{Error As a Function of Iteration Count. Oscillatory Components Are Removed Rapidly, Leaving Smooth Components \cite{Briggs2000}}
   \label{fig:MGerrorExample}
\end{figure}

The idea of multigrid methods is to take advantage of the smoothing effect by making smooth error look oscillatory so it can be more easily removed. Error that is low frequency on a fine grid can be mapped onto a coarser grid where it is oscillatory. This change can be thought of as increasing the number of oscillations per grid point. Look at Figure~\ref{fig:FourierGridError} for an example. When $n$ = 12, the maximum number of half-sine waves is 12, so $k$ = 4 is relatively smooth. When mapped to $n$ = 6, $k$ = 4 is relatively oscillatory. 
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.55\textwidth, height=0.33\textheight] {FourierGridError}
   \end{center}
   \caption{Relative Oscillation of a Fourier Mode Mapped Between Two Grids \cite{Briggs2000}}
   \label{fig:FourierGridError}
\end{figure}

Using this information, multigrid methods can be understood. The error is mapped from a fine grid to a coarser grid. A smoother is applied on the coarse grid to remove the newly oscillatory error components. The result is mapped back to the fine grid and used to correct the solution there. A few more relaxations are done back on the fine grid. That whole process is called a v-cycle, which is described in Algorithm~\ref{algo:MG}. A call to this method is symbolized as $v^h \leftarrow \ve{G}(v^h, b^h)$ and is effectively doing $v^{h} \approx \ve{G}^{-1}b^{h}$.%
\begin{algorithm}
  \caption{ Multigrid v-cycle: $v^h \leftarrow \ve{G}(v^h, b^h)$}
  \label{algo:MG}
  \begin{list}{}{\hspace{2.5em}}
    \item Relax $\nu_1$ times on $\ve{A}^h u^h = b^h$ on the fine grid $\Omega^h$ using initial guess $v^h$.
    \item Compute the residual, $r^h = b^h - \ve{A} v^h$. 
    \item Using some restriction operator, $\ve{R}_h^{2h}$, restrict the residual to a coarser grid: $r^{2h} =  \ve{R}_h^{2h} r^h$. 
    \item Solve the residual equation, $\ve{A}^{2h} e^{2h} = r^{2h}$, on coarse grid $\Omega^{2h}$. 
    \item Using some prolongation operator, $\ve{P}_{2h}^h$, prolong (interpolate) the coarse grid error back to a finer grid: $e^h = \ve{P}_{2h}^h e^{2h}$. 
    \item Add the error to the fine grid guess: $v^h \rightarrow v^h + e^h$. 
    \item Relax $\nu_2$ times on $\ve{A}^h u^h = b^h$ on $\Omega^h$ to get an improved solution. 
   \end{list}
\end{algorithm}
The details of how restriction and prolongation are done are problem dependent. Simple iterative schemes can have very simple operators while more complex schemes may require more complex mappings. Other implementation choices are what relaxer to use and how many relaxations to do on each grid.

There are many variations of how multigrid methods are put together. All methods, however, are built on the basic v-cycle correction scheme. A V-cycle is an extension of the v-cycle. Instead of only using two grids, many grids are used. The problem is restricted from grid to grid until it is on some grid which is coarse enough to directly invert the equations. Then the errors are prolongated back up the chain, continuously correcting on finer grids, until the finest grid is reached. Multigrid methods are differentiated by how many times the v-cycle is done and how many grids are used. A five-grid V-cycle can be seen in Figure~\ref{fig:Vcycle} (a). A W-cycle is when V-cycles are repeated to further improve the answer. An example can be seen in Figure~\ref{fig:Vcycle} (b). 
\begin{figure}
    \begin{center}
      \includegraphics [width=0.7\textwidth, height=0.7\textheight, angle=180 ] {multigridFig}
   \end{center}
   \caption{Grid Schedule for (a) a V-cycle, (b) a W-cycle, and (c) a Full Multigrid Pattern \cite{Briggs2000}}
   \label{fig:Vcycle}
\end{figure}

Multigrid can also be used to obtain an initial guess where much of the smooth error has been removed. This is called nested iteration, and the iterations begin on the coarsest grid and move up to the finest gird. If nested iteration is combined with V-cycles, the full multigrid method (FMG) is obtained, as seen in Figure~\ref{fig:Vcycle} (c). A call to any of the combinations of v-cycles is denoted $v \leftarrow \ve{G}(v, b)$. The optimal combination of grids and cycles may depend on problem type. The addition of more grids and cycles will reduce error, but at an added cost. 

Multigrid methods can be thought of as stationary iterative schemes that can be used alone or as accelerators for other methods. In the past these methods were highly problem specific and only applied to second-order elliptic PDEs. Over time they have been extended to different problem types, geometries, and discretizations \cite{Benzi2002}. 

%-------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------
\section{Past Work}
This section discusses some past work from most of the categories of preconditioners that were presented above. The focus is on methods that have been used to precondition Krylov solvers and on multigrid methods. This section is intended to illustrate the need for preconditioning Krylov methods in transport and to demonstrate the new preconditioner's originality. 

When using Krylov methods, how well a preconditioner is going to do for a certain problem cannot be known a priori. At this time there are no set methods or procedures for predicting preconditioner behavior with Krylov methods. As a result preconditioning Krylov methods is somewhat ``guess and check.'' Despite this, the reward of preconditioning Krylov methods ensure that they are an essential part of their practical use. 

%-------------------------------------------------------------------------------------------------------
\subsection{Rebalance}
CMR has been widely used in transport codes since at least the 1970s and new variants of it are being applied to Krylov solvers \cite{Dahmani2002}, \cite{Yamamoto2005}. For example, Dahmani et.\ al.\ investigated using GMRES and preconditioned GMRES for solving the 3-D transport equation using the method of characteristics (MOC) in 2005. Self-collision rebalance (SCR) was used as a left preconditioner. SCR uses the probability of a region scattering particles to itself to rebalance the energy distribution in each region. MOC is quite different from \Sn and requires a specific derivation to be able to use GMRES \cite{Dahmani2002}. 

%-------------------------------------------------------------------------------------------------------
\subsection{Incomplete Factorizations}
Incomplete factorizations have also been used to precondition Krylov methods, though not as successfully for large transport problems. Patton and Holloway investigated the use of a variety of preconditioners for GMRES to solve the multi-group, diamond-difference, 1-D, \Sn equations. They compare matrix-based and physics-based right preconditioners. A variety of factorization methods, the matix-based preconditioners, were considered: ILU(0), modified ILU(0) (MILU), ILU($\tau=10^{-4}$), ILU($p=10$), and ILU($\tau=10^{-4}, p=10$) where $\tau$ is the drop tolerance and $p$ indicates the amount of fill-in allowed. A single integer indicates the fill-in limit. 

The bandwidth of $\ve{A}$ increases as the number of energy groups and/or discrete ordinates increases \cite{Patton2002}. The bandwidth of a matrix $\ve{A}=(a_{ij})$ is the maximum value of $|i-j|$ such that $a_{ij}$ is nonzero \cite{Wolfram2011}. The computational time required by ILU factorization increases with the bandwidth of $\ve{A}$. This makes factorization methods unattractive as accelerators for finely discretized problems. This prompted Patton and Holloway to consider physics-based methods. 

A matrix splitting was chosen such that the original problem formulation is written as:
%
\begin{equation}
  \ve{L}\psi - \ve{S}_{in}\psi = \ve{Q} \:,
\end{equation}
%
where $\ve{L}$ is the streaming plus removal term, $\ve{S}_{in}$ is the inscattering source, and $\ve{Q}$ contains the external source. Patton and Holloway use $(\ve{L} - \ve{S}_{down})^{-1}$ as the preconditioner, where  $\ve{S}_{down}$ is the downscatter part of the $\ve{S}$ matrix. They use source iteration as the within-group solver such that each new guess is obtained by applying $(\ve{L} - \ve{S}_{down})^{-1}(\ve{S}_{in} - \ve{S}_{down})$ to the old guess. For the 1-D case they found that this physics-based preconditioner is faster than the matrix-based factorization preconditioner, but that DSA is faster than both \cite{Patton2002}. This is not surprising for the 1-D case with diamond difference. 

In 2004 Chen and Sheu compared preconditioned conjugate gradient methods with SOR for 3-D, multigroup neutron transport. They used ILU and MILU to precondition both conjugate gradient squared (CGS) and BiCGSTAB. They chose these iterative techniques because they have good residual error control procedures that give good convergence rates in general \cite{Chen2004}.

Kozlowski, Downar, and Lewis investigated a Krylov preconditioning method for the multi-group $SP_3$ transport equations. This method involves reordering the fluxes to facilitate the use of block ILU as the preconditioner. This idea worked well for small problems, but may require too much storage for large problems \cite{Kozlowski2003}.

%-------------------------------------------------------------------------------------------------------
\subsection{Synthetic Acceleration}
DSA has been under continuous development since Alcouffe's 1977 paper. For example, a recent development string began when Wareing, Larsen, and Adams developed a simple DSA scheme for bilinear discontinuous (BLD) discretizations schemes in 2-D. An unconditionally efficient multigrid technique for solving these 2-D, BLD equations was derived by Morel, Dendy, and Wareing. This was later extended to bilinear nodal differencing and bilinear characteristic differencing \cite{Adams2002}. New versions of DSA are applied to Krylov solvers in current transport codes.

Very recently Rosa et.\ al.\ performed detailed Fourier Analysis on TSA combined with Inexact Parallel Block-Jacobi (IPBJ) splitting applied to the one- and two-dimensional transport cases. They noted that both experience in the nuclear community and analytical work have shown that solution methods, such as GMRES(m) can stagnate for problems containing optically thin spatial regions. Rosa et.\ al.'s analysis and results show that using modified TSA improves the spectral properties such that convergence can be obtained using a relatively small m when using GMRES(m) \cite{Rosa2010}.

\subsubsection{DSA in Denovo}
Denovo has the option to use DSA to precondition the within-group transport equation, $\bigl(\ve{I} - \ve{DL}^{-1}\ve{MS}_{gg}\bigr) \phi_{g} = \ve{DL}^{-1}Q_{g}$, which works very well when using a Krylov method. High-frequency error modes are what often cause instability in DSA. Krylov iteration, like iterative methods in general, will rapidly damp such oscillatory modes. Eliminating the high-frequency error enables the removal of the consistency requirement as DSA is not likely to fail \cite{Evans2009d}. This means DSA can be applied successfully for a variety of spatial discretizations. 

The DSA-preconditioned one-group equation is:
%
\begin{equation}
  \bigl(\ve{I} + \ve{PC}^{-1}\ve{RS}\bigr) \bigl(\ve{I} - \ve{DL}^{-1}\ve{MS}\bigr) \phi = \bigl(\ve{I} + \ve{PC}^{-1}\ve{RS}\bigr)\ve{DL}^{-1}\bar{Q} \:. 
  \label{DSA1group}
\end{equation}
%
Here $\ve{C}$ is the diffusion operator defined in Appendix~\ref{sec:AppendixA}; $\ve{R}$ is the restriction operator, which maps the transport solution onto the diffusion vector; and $\ve{P}$ is the projection operator, which maps the diffusion vector onto the transport solution. Denovo does not actually form these operators, instead it solves the diffusion equation and updates the $\phi_{00}$ moments. In practice this means that for a Krylov iteration
%
\begin{equation}
  \ve{C}z = \ve{RS}\bigl(\ve{I} - \ve{DL}^{-1}\ve{SM}\bigr) v
\end{equation}
%
is solved for each group, where $v$ is the iteration vector. DSA was found to be useful for some problems. In Denovo DSA has been found to be beneficial for diffusive problems with high scattering ratios that are close to being isotropic \cite{Evans2009d}. This is consistent with experiences of the wider nuclear community.

%-------------------------------------------------------------------------------------------------------
\subsection{Multigrid Methods}
Beginning in the late 1980s, the nuclear community started using spatial \mg and/or angular \mg as both solvers and preconditioners. The first use of spatial \mg for transport equations in 1-D and 2-D was investigated by Nowak et.\ al. Since that time \mg has been used in multiple dimensions, for both isotropic and anisotropic scattering, and for various spatial discretizations \cite{Adams2002}. Some highlights from recent work are discussed below. All are applied to the \Sn neutron transport equation unless otherwise noted. 

In 1996 Sjoden and Haghighat used a simplified spatial \mg method that does not use the residual as a solver for the 3-D, parallel code PENTRAN \cite{Sjoden1996}. In 1998 multigrid in space and multigrid in angle were used as preconditioners for Krylov methods and were tested for the 1-D, one-group, modified linear discontinuous (MLD) neutron transport equations by Oliveira and Deng. They looked at isotropic scattering without absorption, isotropic with absorption, and anisotropic cases. They had better results with multigrid than when using ILU as a preconditioner \cite{Oliveira1998}.

In 2007 Chang et.\ al.\ used 2-D spatial \mg for the isotropic scattering case with corner balance finite difference in space and a four-color block-Jacobi relaxation scheme. A bilinear interpolation operator, and its transpose, were used for grid transfer. The method had some trouble with heterogeneous problems. The authors assert their algorithm is parallelizable \cite{Chang2007}.

In 2010 Lee developed a method to do \mg in space and angle simultaneously for two and three dimensions, isotropic and anisotropic scattering, one energy group, and a variety of spatial discretizations. The method can perform \mg only space, only angle, or some combination there of. It also handles thick and thin cells \cite{Lee2010}.

This list is hardly comprehensive, but is somewhat representative of new and recent developments in this area. The author was unable to find any cases where \mg was used in the energy variable either as a solution technique or as a preconditioner. 

\subsubsection{Two-Grid Acceleration}
The two-grid acceleration method developed by Adams and Morel is one of the earlier spatial \mg methods, which has been built upon by others \cite{Adams1993}. It is intended to accelerate convergence of the outer iterations for the transport equation when upscattering is present; the outer iteration method is Gauss Seidel. The original work was done for slab geometry with linear discontinuous (LD) discretization; it is a one-grid, one-cycle scheme in angle. The general approach is expounded upon here to provide an example of \mg methods applied to the transport equation and because a variation of this method is used in Denovo. 

Adams and Morel create a within group error equation by subtracting the GS equation from the transport equation. If the error in iteration $k$ is $\epsilon^k$ and the $l$th moment of the residual is $R_l^k$, then for group $g$ in 1-D this gives:
%
\begin{align}
   \mu \frac{\partial \epsilon_{g}^{k+1}(\mu)}{\partial x} + \Macro_{t,g} \epsilon_{g}^{k+1}(\mu) &= \sum_{l=0}^{L}\frac{2l+1}{4\pi} \bigl( \sum_{g'=1}^{g}\Macro_{s,g' \to g, l} \epsilon_{g',l}^{k+1} 
   +  \label{eq:GSerror} \\
   &\sum_{g'=g+1}^{N}\Macro_{s,g' \to g, l} \phi_{g',l}^{k} + R_{g,l}\bigr) P_{l}(\mu) \:, \nonumber \\
  \epsilon_{g}^{k+1}(\mu) &= \psi_{g}(\mu) - \psi_{g}^{k+1}(\mu) \:, \\
  \epsilon_{g',l}^{k+1} &= 2\pi \int_{-1}^{1}\epsilon_{g'}^{k+1}(\mu') P_{l}(\mu') d\mu' \:, \\ 
  R_{g,l}^{k+1} &=  \sum_{g'=g+1}^{N}\Macro_{s,g' \to g, l} \bigl( \phi_{g,l}^{k+1} - \Macro_{g',l}^{k} \bigr) \:.
\end{align}
%
The diffusion approximation is applied to Equation \eqref{eq:GSerror}, giving a coarse grid equation. The coarse grid equation is then solved on the coarse grid for the error, which is in turn added to the flux approximation. 

Next, it is assumed that the zeroth moment of the error is a product of a spectral shape function, $\xi_g$ and a space-dependent modulation function, $E(x)$:
%
\begin{align}
  \epsilon_{g,0}^{k+1}(x) &= E(x)\xi_{g} \:, \text{ and} \label{eq:errorExpand} \\
  \sum_{g=1}^{N} \xi_{g} &= 1 \:.
\end{align} 
%
The spectral shape function corresponds to the slowest converging error mode, i.e.\ the mode to be eliminated. To find the shape function, Fourier analysis is performed on the GS iterative method. The zeroth moment of the cross sections is used to form the Fourier matrix, and then an eigenproblem can be formed. The spectral radius of the isotropic GS matrix is the eigenvalue, and the corresponding eigenvector is the shape function. Because the shape function is dependent on materials, one such calculation must be done for each material region. Note that by using the zeroth moment only the isotropic component of the solution is accelerated.

The coarse grid diffusion equations that result from Equations \eqref{eq:GSerror} and \eqref{eq:errorExpand} differ slightly in form from the standard diffusion equation in that there is an extra term containing the gradient of the shape function. This is zero in homogeneous regions, but undefined at material interfaces. Adams and Morel found that neglecting the gradient term all together still gave good results for their test problems. This may not be true for more complex cases.

The two-grid method requires the diffusion operator to be consistent with the transport operator, just like in DSA. This requirement can be difficult to meet for multi-dimensional problems, particularly for some spatial discretizations. 

\subsubsection{Two-Grid in Denovo}
\label{sec:TTG}
Denovo has a two-grid acceleration scheme based on the one developed by Adams and Morel. As noted above, the iteration procedure uses a collapsed one-group diffusion equation to correct the low-order Fourier modes \cite{Adams1993}. Because of the consistency requirement for the discretization of the diffusion operator in multi-dimensional and multi-material problems this method can fail for systems of interest \cite{Evans2009d}. 

The original two-grid method was modified by Evans et.\ al.\ to make it applicable for the desired cases by using a one-group transport equation instead of the diffusion equation. The modified method is called transport two-grid (TTG). Adams and Morel showed that the slowest converging spatial modes are diffusive and can be exactly computed in the infinite homogeneous case. To preserve this, the TTG method should give the correct error estimation in that limit \cite{Evans2009d}. 

To preserve this, the cross sections used in TTG are calculated to give the same energy-collapsed cross sections as the diffusion equation for the infinite homogeneous case, as seen in Equations \eqref{TTGxsecs1} and \eqref{TTGxsecs2}. TTG solves a one-group transport equation for the low-order error in each GS iteration:
%
\begin{align}
  \ve{\hat{\Omega}} \cdot \nabla \psi_{\epsilon} &+ \bar{\Macro}\psi_{\epsilon} = \frac{1}{4\pi}\bar{\Macro}_{s}\phi_{\epsilon} + \frac{1}{4\pi}\bar{R} \:, \\
  \bar{\Macro} &= \frac{1}{\sum_{g=g_1}^{g_2} \frac{1}{\Macro^g}\zeta^g} \label{TTGxsecs1} \:,\\
  \bar{\Macro}_{s} &= \frac{1}{\sum_{g=g_1}^{g_2} \frac{1}{\Macro^g}\zeta^g} - \sum_{g=g_1}^{g_2} \bigl(\Macro^g\zeta^g - \sum_{g'=g_1}^{g_2} \Macro_{s0}^{gg'} \zeta^{g'} \bigr) \label{TTGxsecs2} \:. 
\end{align}
%
The spatial components of the error are $\psi_{\epsilon}$ and $\phi_{\epsilon}$; $\bar{R}$ is the residual. Just as in the original method, it is assumed that the error is separable in space and energy at each iteration: $\epsilon_g^k = \phi_g - \phi_g^k = \phi_{\epsilon}(\vec{r})\zeta^g$, where $\zeta^g$ is a material-dependent spectral function \cite{Evans2009d}. 

To execute the TTG scheme in Denovo, a transport sweep in conducted in each group, a residual is calculated, the low-order transport solve described above is performed, an error form of the transport sweep is conducted, and the scalar flux is updated. All of that can be seen in the following equations: 
\begin{align}
  \ve{L}_g \psi_g^{k+\frac{1}{2}} &= \ve{M}\bigl(\ve{S}_{gg}\phi_g^{k+\frac{1}{2}} + \sum_{g'=g_1}^{g-1} \ve{S}_{gg'}\phi_{g'}^{k+\frac{1}{2}} + \sum_{g'=g+1}^{g_2}\ve{S}_{gg'}\phi_{g'}^k \bigr) + q_{eg}  \:, \\
  R^{k+\frac{1}{2}}_g &= \ve{M} \sum_{g'=g+1}^{g_2}\ve{S}_{gg'} \bigl( \phi_{g'}^{k+\frac{1}{2}} - \phi_{g'}^k \bigr) \:, \qquad l = m = 0 \:, \\
  \bar{\ve{L}}\psi_{\epsilon} &= \ve{M\bar{S}} \phi_{\epsilon} + \bar{R} \:, \qquad l = m = 0 \:, \label{TTGerrorEqn} \\
  \phi_g^{k+1} &= \phi_{g}^{k+\frac{1}{2}} + \phi_{\epsilon}\zeta^g \:, \qquad l = m = 0 \:.
\end{align}
The operators with over-bars use the collapsed cross sections given in Equations \eqref{TTGxsecs1} and \eqref{TTGxsecs2}, and $\bar{R} = \sum_{g=g_1}^{g_2} R_g^{k+\frac{1}{2}}$. 

The $\zeta_g$ term is calculated from the eigenvalue problem obtained through Fourier analysis of the GS method using isotropic scattering, just like the original two-grid method:
%
\begin{equation}
  \bigl(\ve{T}_{\Macro} - \ve{S}_L + \ve{S}_D\bigr)^{-1} \ve{S}_U\zeta = \rho\zeta \:,
\end{equation}
where $\ve{T}_{\Macro}$ is the diagonal, total cross section matrix. As with the original two-grid method, the TTG method is limited to correcting only the isotropic flux moments. This is an adequate limitation as these moments are dominant in thermal groups where upscattering is most prevalent\cite{Evans2009d}.   

The additional cost of the method is like solving one extra group, so for cases with many upscattering groups the cost can be amortized. The acceleration equation can be preconditioned with DSA for additional speed. Finally, a reduced quadrature set can be used when solving Equation~\eqref{TTGerrorEqn} to limit the cost. Some test problems have shown TTG to be quite effective in improving the speed of convergence for upscatter problems when compared to unaccelerated GS \cite{Evans2009d}.

%-------------------------------------------------------------------------------------------------------
Clearly, a wide variety of preconditioning techniques have been applied to the neutron transport equation. It is worth noting that many of these methods are dependent upon the choice of spatial discretization employed, only apply to within group iterations, or have other important limitations. Preconditioning Krylov methods for solving the neutron transport problem is an active and vital area of research where much progress has been made and in which there is still much room for development. 

%-------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------
\section{Multigrid in Energy}
Preconditioning is a very important part of increasing the robustness of Krylov methods. This is particularly true in this work for two reasons. The first is that the multigroup Krylov solver can create very large Krylov subspaces because it forms the subspaces with multiple-group-sized vectors. As a result, any reduction in iteration count will have a large benefit in terms of both memory and cost-per-iteration. The second is that preconditioning is needed to compensate for the ill-conditioned systems created by RQI so that the eigenvector can be converged. 

A new physics-based preconditioner has been added to Denovo to improve the performance of the Krylov solves. It does multigrid in energy. Choosing a physics-based preconditioner supports the goal of accelerating a code that solves a specific equation, not developing an all purpose preconditioner. It makes sense to take advantage of information specific to the neutron transport equations. The past work sections above show physics-based preconditioners often provide more benefit than matrix-based methods. Further, the matrix $\ve{A}$ is never formed in Denovo making matrix-based preconditioners impossible anyway. 

The concept of multigrid in energy is that the grid is in energy, not space or angle like methods used in other work. The motivation behind this choice is that multigrid methods have been successful at accelerating the transport equation. Using grids in energy rather than space or angle is partially for simplicity, grids in the other dimensions can be complex and expensive, and partially because it seemed like it might work very well. Finally, the convergence behavior of the Krylov iterations inside RQI looks like a very good candidate for multigrid methods. 

%-------------------------------------------------------------------------------------------------------
\subsection{Method}
To make energy grids, the energy group structure is coarsened so each lower grid has fewer groups on it. The finest grid is the input energy structure and the coarsest grid has one or a few groups. Each level has half as many groups as the previous level, rounded up if applicable. This is conceptually straightforward because the equations are only one-dimensional in energy and energy groups can be combined (restricted) and separated (prolonged) linearly. 

The implemented restriction operator is a simple averaging scheme. Neighboring fine data are averaged together to make coarse data. Recall that for a grid with spacing $h$, $2h$ is the next-coarser grid. The moments are restricted as $\phi_{g}^{2h} = \frac{1}{2}(\phi_{2g}^{h} + \phi_{2g+1}^{h})$ for $g = 1,...,G-1$. If there are an odd number of groups the lowest energy group is just copied. This scheme was chosen so that the thermal energy groups would retain more granularity, which should improve accuracy for thermal reactors. The moments are restricted every time there is transfer from a given grid to a coarser grid.

The cross sections are restricted from the finest to the coarsest grid during problem initialization and they do not change thereafter. The total and fission cross sections are restricted in the same way as the moments. Scattering is slightly more complicated since it is in two dimensions, $g$ and $g'$. When there are an even number of groups all cross sections are treated the same way. For $g' = 1,..., G'$ and $g = 1, ..., G$ where $G$ is the number of groups on coarse grid and $2G+1$ is the number of groups on the fine grid,
\begin{equation}
  \Sigma_s^{2h}(g,g') = \frac{1}{4}[\Sigma_s^{h}(2g,2g') + \Sigma_s^{h}(2g+1,2g') + \Sigma_s^{h}(2g,2g'+1) + \Sigma_s^{h}(2g+1,2g'+1)] \:. 
  \label{eq:XSSeven}
\end{equation}
% 
Unless there is upscattering in every group, some of the entries in Equation~\eqref{eq:XSSeven} will be zero. When there are an odd number of groups Equation \eqref{eq:XSSeven} is used until $g=G-1$ and $g'=G'-1$. For the last group
  \begin{align}
    \Sigma^{2h}_s(G,g') &= \frac{1}{2}[\Sigma^{h}_s(2G,2g') + \Sigma^{h}_s(2G,2g'+1)] \qquad \text{for } g' = 0,...,G'-1 \:,\nonumber \\
    \Sigma^{2h}_s(g,G') &=  \frac{1}{2}[\Sigma^{h}_s(2g,2G') + \Sigma^{h}_s(2g_1,2G')] \qquad \text{for } g  = 0,...,G-1 \:,\nonumber \\
    \Sigma^{2h}_s(G,G') &= \Sigma^{h}_s(2G,2G') \nonumber \:.
  \end{align}

To prolong from a coarse grid to a fine, the points that line up between the grids are mapped directly, $\phi_{2g}^{h} = \phi_{g}^{2h}$. To fill in the intermediate points on the fine grid, the adjacent coarse values are linearly interpolated, $\phi_{2g+1}^{h} = \frac{1}{2}(\phi_{g}^{2h} + \phi_{g+1}^{2h})$. The moments are prolonged every time there is a transfer from a given grid to a finer grid. Since cross sections do not change they are never prolonged. 

The choice of how to restrict and prolong between grids is hard-coded into the preconditioner. There are other multigrid method choices that are easier to vary and some of these are taken as user inputs. The number of V-cycles is a user input. One V-cycle goes from the finest grid to the coarsest grid and back up, but the user can choose to do more than one V-cycle per application. That means that for one application of the preconditioner ($v \leftarrow \ve{G}(v,b)$), any number of the large Vs seen in Figure~\ref{fig:Vcycle} are concatenated together. One cycle looks like one V, two cycles looks like one W, and so on. The default number of V-cycles is 2. The idea is that each additional V-cycle will remove more error. 

At this time the depth of the V-cycle is prescribed by the number of groups. This is set such that the grids will be coarsened until there is only one energy group. The number of grids needed is given by \cite{BinaryTree2011}
\begin{equation}
  \text{floor}\bigl( \frac{ln(G-1)}{ln(2)}\bigr) + 2 \:.
  \label{eq:NumGrids}
\end{equation}
%
Note, this is handled differently with energy sets as discussed below. The depth of the cycle is an option that could be changed in the future if it is found that restricting down to only one group is unnecessary, particularly if there are a large number of groups. Investigating the optimal V-cycle depth is an interesting issue, but beyond the scope of this work. 

On each grid that makes up a V-cycle some number of relaxations are performed. The number of relaxations per level, $\nu$, is a user input choice with a default of 2. Doing more relaxations per grid should remove more error overall. The relaxation method selected is weighted Richardson iteration. In Equation \eqref{eq:relax} $k$ is the iteration index, where $k = 1, ..., \nu$, and $\omega$ is the weighting parameter. This formulation takes the identity portion of $\ve{A}$ into account and $b$ is either $\ve{TM}q_{e}$ or $\frac{1}{k}\ve{TMF}\phi$.
%
\begin{align}
  \phi^{k + \frac{1}{2}} &= \ve{TMS}\phi^{k} + b \:, \text{and} \nonumber \\
  \phi^{k+1} &= \omega \phi^{k + \frac{1}{2}} + (1 - \omega) \phi^{k} \:, \text{ which can be combined to give } \nonumber \\
  \phi^{k+1} &= \omega \ve{TMS}\phi^{k} + \omega b + (1 - \omega) \phi^{k} \:. 
  \label{eq:relax}
 \end{align}
 
The weight is also selected by the user and defaults to 1. In general the weight should be chosen such that $1 \le \omega < 2$. With over relaxation methods, increasing the weight typically reduces the number of required iterations up until some turning point that is problem dependent, though often close to 2, where the iteration count increases again. While this is the case in methods like successive over relaxation, it may not hold true when applied to weighted Richardson inside of a preconditioner since preconditioner behavior is often a little different than when the method is used as an ordinary solver.

An important principle to keep in mind is that the preconditioner is only attempting to roughly invert $\ve{A}$. It therefore might be a good idea to use simplifications besides just using weighted Richardson instead of a Krylov method. In this vein there is an option to use a smaller angle set in the preconditioner than the rest of the solution. For example, the whole problem can be solved at $S_{10}$, but the preconditioner would only use $S_{2}$. There is an input option to specify what to use in preconditioner; the default is whatever is being used by the entire problem. This is only available for problems with vacuum boundary conditions at this time. 

Recall that right preconditioning is applied as $\ve{A} \ve{G}^{-1} \ve{G} \phi = b$, where $\ve{A} = \ve{I} - \ve{TMS}$. To actually implement this in Denovo, the problem must be broken up into several steps: 
%
\begin{align}
  \text{Define} \qquad y &= \ve{G}\phi \:;\\
  \text{with a Krylov method solve} \qquad \ve{AG}^{-1}y &= b \:. \\
  \text{After finding }y\text{, the final step is} \qquad \phi &= \ve{G}^{-1}y \:.
\end{align}
%
Because the Krylov solvers are provided by an external library they cannot be modified easily, so carrying out the second step means the preconditioner must be applied to the iteration vectors handed to the solver. Equations~\eqref{eq:invertG}, \eqref{eq:ApplyA}, and \eqref{eq:findPhi} show how this works.  

Let $v^{j}$ be an iteration vector that represents $y$, and let $z^{j}$ be an intermediate iteration vector. At each step below the equations are shown three ways to make clear what is going on algorithmically: the equation being solved, the symbolic representation of the outcome, and the way this is written in multigrid syntax. The first thing is to apply the preconditioner to the intermediate vector: %to affect the inversion of $\ve{G}$,
%
\begin{align}
  \ve{G}z^{j} &= v^{j} \:,  \label{eq:invertG} \\
  z^{j} &\approx \ve{G}^{-1}v^{j} \:, \nonumber \\
  z^{j} &\leftarrow \ve{G}(z^{j}, v^{j}) \:. \nonumber
\end{align}
%
Next, apply $\ve{A}$ to $z^{j}$ and set it equal to $v^{j+1}$; $y$ is also $v^{j+1}$:
\begin{align}
  v^{j+1} &= \ve{A}z^{j} \:,   \label{eq:ApplyA} \\
  v^{j+1} &\approx \ve{AG}^{-1}v^{j} \:, \nonumber \\
  y &= \ve{A}[z^{j} \leftarrow \ve{G}(z^{j}, v^{j})] \:. \nonumber
\end{align}
%
To get the final $\phi$ from $y = \ve{G}\phi$, apply the preconditioner again to an iteration vector $w$:
%
\begin{align}
  \ve{G}w^{j} &= y \:,   \label{eq:findPhi} \\
  w^{j} &\approx \ve{G}^{-1}y \:, \nonumber \\
  \phi &= w^{j} \leftarrow \ve{G}(w^{j}, y) \:. \nonumber
\end{align}

The operator is used within the preconditioner to compute the residual, $r = \ve{A}\phi - b$. That is the only part of the preconditioning algorithm that ``applies $\ve{A}$''.  When doing RQI the default behavior is to use an unshifted operator in the preconditioner. There is an option to use the shifted operator instead. In that case $\mathbf{S}$ becomes $\tilde{\ve{S}} = \ve{S} + \rho\ve{F}$ and the right hand side becomes $(\frac{1}{k} - \rho)\ve{TMF}\phi$. The use of the shifted operator can be turned on through an input option.

An important attribute of this preconditioner is that it is parallelizable in energy because it can use the energy sets introduced earlier in this work. There are two ways to handle energy grids and energy sets together. One way is to restrict from $G$ groups down to $1$ group just like there weren't any energy sets. This requires cross-set communication as soon as there are fewer groups than sets. This also causes some logistical difficulties related to what data is held by which sets at various points in the calculation. 

The other way is to prohibit cross-set communication by having each set do its own ``mini'' V-cycle. Each set restricts, prolongs, and relaxes on only its groups. Instead of restricting to one group overall, each set restricts to one or two group(s), giving approximately num\_sets groups overall. This strategy means there must be at least two groups on every set. The number of grids needed is determined by the set with the minimum number of groups since it will be the first to reach a grid with one group. To choose the number of levels, Algorithm~\ref{algo:multisetsGrids} is used. 
%
\begin{algorithm}
  \caption{ Setting the Number of Grids When There Are Energy Sets}
  \label{algo:multisetsGrids}
   num\_levels = 1, num\_local\_min = floor$\bigl( \frac{\text{num\_groups}}{\text{num\_sets}}\bigr)$ \\
   while (num\_local\_min $>$ 1)
  \begin{list}{}{\hspace{2.5em}}
    \item num\_levels = num\_levels + 1
    \item num\_groups = (num\_groups + 1) / 2
    \item num\_local\_min = floor$\bigl( \frac{\text{num\_groups}}{\text{num\_sets}}\bigr)$
   \end{list}
\end{algorithm}

The communication costs and logistical complications of the first strategy seem likely to overwhelm the benefit gained by going to one group instead of num\_sets groups. The second strategy was chosen because it involves much less communication and overhead cost. The validity of this choice will be commented upon in the results section. 

With the implemented energy set strategy there are tradeoffs between the number of sets and number of grids for a fixed number of groups. When there are more sets, more cores can be used at one time and wall time should decrease. When there are fewer sets each V-cycle can go deeper so the preconditioner should be more effective, which will reduce iteration count and hopefully decrease wall time. This tradeoff is also investigated.

%-------------------------------------------------------------------------------------------------------
\subsection{Results}
Investigating the performance of the preconditioner is not simple because there are so many variables. The varsiety of knobs to turn are grouped into three categories: 1) problem and solver type, 2) preconditioning parameters, 3) number of groups and sets. The problem types and solvers are fixed source, eigenvalue with power iteration, and eigenvalue with Rayleigh quotient iteration. All were solved with the multigroup Krylov solver unless otherwise noted. The preconditioning parameters are weight, number of V-cycles, and number of relaxations per level. The group consideration deals with whether there are many or few groups and how many groups per set were used when doing multisets. Calculations were done to investigate all of these combinations.

\subsubsection{Fixed Source}
Shows that preconditioner reduces multigroup Krylov iteration count, useful even for these problems. True for reflecting and vacuum, few and many groups.

%-------------------------------------------------------------------------------------------------------
* parameter variation with fixed source vacuum, 10 groups (FxdSrcTst\_vac)
one material, 10 groups with 5 upscattering, first 3 groups have an isotropic source, $P_{0}$, $S_{4}$, 3 $\times$ 3 $\times$ 3 grid, vacuum boundaries, tolerance and upscatter tolerance of 1 $\times$ 10$^{-6}$. This is a fixed source manager unit test run on my mac with the debug version; no timing information is available. GMRES with multigroup Krylov solver. 

See the results for varying the weight with 1 relaxation per level and 1 V-cycles in the top plot in Figure~\ref{fig:FxdSrcVac}. The results for varying with number of relaxations per grid and the number of V-cycles with the weight fixed at one can be seen in the bottom plot of that figure. When the preconditioning parameters were really cranked up nothing changed, see Table~\ref{table:FxdSrcTstVacOther}.
%
\begin{table}[!h]
\caption{Small Fixed Source Vacuum Boundary, With Much Preconditioning}
\begin{center}
\begin{tabular}{c c c c}
\hline
Weight & Relaxations & V-cycles & Iterations \\[0.5ex]
\hline
0  & 0 & 0 & 10 \\
2 & 2 & 2 & 4 \\
2 & 3 & 3 & 4 \\
1.3 & 4 & 4 & 4 \\
1.3 & 10 & 10 & 4 \\
\hline 
\end{tabular}
\end{center}
\label{table:FxdSrcTstVacOther}
\end{table}
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.7\textwidth, height=0.8\textheight] {FxdSrcVac}
   \end{center}
   \caption{Preconditioning Parameter Studies for the Small Fixed Source Problem with Vacuum Boundaries}
   \label{fig:FxdSrcVac}
\end{figure}
%-------------------------------------------------------------------------------------------------------

* parameter variation with fixed source reflecting, 10 groups (FxdSrcTst\_refl)
one material, 10 groups with 5 upscattering, first 3 groups have an isotropic source, $P_{0}$, $S_{4}$, 3 $\times$ 3 $\times$ 3 grid, reflecting boundaries, tolerance and upscatter tolerance of 1 $\times$ 10$^{-6}$. This is a fixed source manager unit test run on my mac with the debug version; no timing information is available. GMRES with multigroup Krylov solver. 

See the results for varying the weight with 1 relaxation per level and 1 V-cycles in the top plot of Figure~\ref{fig:FxdSrcRefl}. The results for varying with number of relaxations per grid and the number of V-cycles with the weight fixed at one can be seen in the bottom plot. 
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.7\textwidth, height=0.8\textheight] {FxdSrcRefl}
   \end{center}
   \caption{Preconditioning Parameter Studies for the Small Fixed Source Problem with Reflecting Boundaries}
   \label{fig:FxdSrcRefl}
\end{figure}
%-------------------------------------------------------------------------------------------------------

* comparison of gs, ttg gs, mg krylov, and preconditioned mg krylov + demo of reduced angle solve for fixed source, 27 groups (FeC\_vac)
The half iron half carbon toy problem from Chapter~\ref{sec:Chp2}. Run on the emac with the debug version of the code. 10 $\times$ 10 $\times$ 10, tolerance and upscatter tolerance of 1 $\times$ 10$^{-6}$, $S_{8}$. A few results comparing solvers can be seen in Table~\ref{table:FeC solvers}. The syntax is that w\# is the weight, r\# is the number of relaxations per level, and v\# is the number of V-cycles, e.g.\ w1r1v1 is one relaxation per level, one V-cycle, and a weight of 1.

This problem also very briefly investigated the option of using a different angle set within the preconditioner. The overall problem was solved with $S_{8}$ and the preconditioner only used $S_{2}$. For the w1r2v2 case the time was reduced from 7.07 $\times 10^{2}$ seconds to 1.57 $\times 10^{2}$ seconds. 
%
\begin{table}[!h]
\caption{Iron Carbon Fixed Source Cube, Solver and Preconditioning Comparison}
\begin{center}
\begin{tabular}{l c l c}
\hline
Solver & GS iters & Krylov & time (s)\\[0.5ex]
\hline
GS &  12 & 1,727 & 1.12 $\times 10^{2}$ \\
GS TTG & 11 & 1687 & 1.99 $\times 10^{2}$  \\
MG Krylov & n/a & 30 & 8.78 $\times 10^{1}$ \\
w1 r2 v2 & n/a & 10 & 7.07 $\times 10^{2}$ \\
w1.3 r4 v4 & n/a & 4 & 1.29 $\times 10^{3}$ \\
\hline
\end{tabular}
\end{center}
\label{table:FeC solvers}
\end{table}
%-------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------

\subsubsection{Power Iteration}
Shows that reducing mg Krylov iterations in multigroup solves is useful for eigenvalue problems. True for few and many groups, reflecting (vacuum to come)

* parameter variation with power iteration reflecting, 7 groups (2D Orhtanc)
Results are in Figure~\ref{fig:2Dc5g7PI}. All $k$ values were within the uncertainty of the benchmark and so are not reported. These were run on the small orthanc cluster at Oak Ridge with 16 cores; 4 x-blocks and 4 y-blocks, 1 z-block and 1 set. The total and upscatering tolerances were 1 $\times 10^{-3}$, with a k tolerance of 1 $\times 10^{-5}$. In the table ``Krylov'' is the total number of Krylov iterations and ``Eigenvalue'' is the total number of Eigenvalue iterations. Two other calculations with a higher level of preconditioning were also done, all preconditioned cases used 31 eigenvalue iterations. When the parameters were w1.4, r2, v2 the number of Krylov iterations were reduced to 438, and the calculation took 1.77 $\times 10^{4}$ seconds. For w1r3v1, 253 Krylov iterations and 2.28 $\times 10^{4}$ seconds were required.
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.7\textwidth, height=0.7\textheight] {2dc5g7PI}
   \end{center}
   \caption{Preconditioning Parameter Studies for the Small Fixed Source Problem with Reflecting Boundaries}
   \label{fig:2Dc5g7PI}
\end{figure}
%------------------------------------------------------------------------------------------------------- 
 
* parameter variation with power iteration reflecting, 7 groups (3D Orhtanc)
The same type of study was done with the 3D benchmark on the oic cluster at Oak Ridge. This used 720 cores with 40 x-blocks, 18 y-blocks and 5 z-blocks. The total and upscattering tolerances were 1 $\times 10^{-4}$, with a k tolerance of 1 $\times 10^{-5}$ unless otherwise indicated. Results seen in Table~\ref{table:3D c5g7}. None of these results were within the uncertainty bounds of the reported benchmark. All results were low by about 0.011.
%
\begin{table}[!h]
\caption{3D C5G7 Benchmark With Power Iteration, Preconditioning Parameter Scoping}
\begin{center}
\begin{tabular}{c c c c l l}
\hline
Weight & Relaxations & V-cycles & Krylov & PI & Time (s) \\[0.5ex]
\hline
0    & 0 & 0 & 1,224 & 32 & 4.46 $\times 10^{3}$ \\
1    & 1 & 1 & 708    & 32 & 2.12 $\times 10^{4}$ \\
1.2 & 1 & 2 & 448    & 32 & 2.38 $\times 10^{4}$ \\
1.2 & 2 & 1 & 448    & 32 & 2.39 $\times 10^{4}$ \\
1.3 & 2 & 2 & 288    & 32 & 2.84 $\times 10^{4}$ \\
1.5 & 3 & 3 & 192    & 32 & 3.73 $\times 10^{4}$ \\
1    & 3 & 3 & 126    & 14$^{*}$  & 4.04 $\times 10^{4}$ \\
1    & 4 & 4 & n/a     & n/a          & exceeded wall time \\
1    & 4 & 4 & n/a     & n/a$^{*}$ & exceeded wall time \\
1.5 & 5 & 5 & n/a     & n/a          & exceeded wall time \\
\hline 
\end{tabular}\\
$^{*}$tol and upscatter tol = 1 $\times 10^{-5}$, $k$ tol = 1 $\times 10^{-3}$.
\end{center}
\label{table:3D c5g7}
\end{table}
%-------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------

\subsubsection{RQI}
Shows that the preconditioner is useful for problems we can solve with rqi, and it makes rqi possible for problems we couldn't solve before. 

* parameter variation with rqi vacuum, 4 groups (RQI\_UnitTest\_vac)
Small RQI unit test with vacuum boundary conditions. This is the same problem that was reported upon in Chapter~\ref{sec:Chp3}. The correct $k$ and flux were found in all cases. The tolerance used to compare the flux to the reference case was $1 \times 10^{-5}$. The results are shown in Table~\ref{table:RQIUnitTestVac}.
%
\begin{table}[!h]
\caption{RQI Unit Test With Vacuum Boundaries, Preconditioning Parameter Study}
\begin{center}
\begin{tabular}{c c c c c}
\hline
Weight & Relaxations & V-cycles & Krylov & RQI \\[0.5ex]
\hline
0    & 0 & 0 & 39 & 6 \\
1    & 1 & 1 & 27 & 6 \\
1.2 & 1 & 1 & 31 & 6 \\
1    & 2 & 1 & 16 & 6 \\
1.2 & 2 & 1 & 19 & 6 \\
1    & 1 & 2 & 16 & 6 \\
1.2 & 1 & 2 & 19 & 6 \\
1    & 2 & 2 & 11 & 6 \\
1.2 & 2 & 2 & 11 & 6 \\
\hline
1    & 2 & 3 & 10 & 6 \\
1.2 & 2 & 3 & 10 & 6 \\
1.3 & 2 & 3 & 10 & 6 \\
1.4 & 2 & 3 & 10 & 6 \\
\hline
1    & 3 & 3 & 6   & 6 \\
1    & 4 & 4 & 6   & 6 \\
1.3 & 4 & 4 & 6   & 6 \\
1    & 5 & 5 & 6   & 6 \\
1.3 & 5 & 5 & 6   & 6 \\
\hline 
\end{tabular}
\end{center}
\label{table:RQIUnitTestVac}
\end{table}
%-------------------------------------------------------------------------------------------------------

* parameter variation with rqi vacuum + shifted operator, 4 groups (RQI\_UnitTest\_vac)
Small RQI unit test with vacuum boundary conditions. This test used the shifted version of the operator in the preconditioner. Unless otherwise noted the correct $k$ and flux were found. The flux checking tolerance was again $1 \times 10^{-5}$. The results are shown in Table~\ref{table:RQIUnitTestVacShifted}.
%
\begin{table}[!h]
\caption{RQI Unit Test With Vacuum Boundaries and Shifted Operator, Preconditioning Parameter Study}
\begin{center}
\begin{tabular}{c c c c c}
\hline
Weight & Relaxations & V-cycles & Krylov & RQI \\[0.5ex]
\hline
0    & 0 & 0 & 39 & 6 \\
1    & 1 & 1 & 20 & 6$^{*}$ \\
1.2 & 1 & 1 & 31 & 7$^{\dagger}$ \\
1    & 2 & 1 & 12 & 6 \\
1.2 & 2 & 1 & 16 & 6 \\
1    & 1 & 2 & 12 & 6 \\
1.2 & 1 & 2 & 16 & 6 \\
1    & 2 & 2 & 10 & 6 \\
1.2 & 2 & 2 & 10 & 6 \\
1    & 2 & 3 & 6   & 6 \\
\hline 
\end{tabular}\\
$^{*}$flux was not correct and $k$ was 0.17632 instead of 0.17528 \\
 $^{\dagger}$flux was not correct and $k$ was 0.17494 instead of 0.17528
\end{center}
\label{table:RQIUnitTestVacShifted}
\end{table}
%-------------------------------------------------------------------------------------------------------

* parameter variation with rqi reflecting, 4 groups (RQI\_UnitTest\_refl)
This is the small RQI unit test with reflecting boundary conditions, the same that was discussed in Chapter~\ref{sec:Chp3}.The flux was not tested against a very tight tolerance, either $1 \times 10^{-2}$ or $1 \times 10^{-3}$ unless otherwise noted. The results are show in Table~\ref{table:RQIUnitTestRefl}. Note that here the max number of Krylov iterations was set to 100.
%
\begin{table}[!h]
\caption{RQI Unit Test With Reflecting Boundaries and Shifted Operator, Preconditioning Parameter Study}
\begin{center}
\begin{tabular}{c c c c c c}
\hline
Weight & Relaxations & V-cycles & $k$ & Krylov & RQI \\[0.5ex]
\hline
0    & 0 & 0 & 2 & 35   & 5 \\
1    & 1 & 1 & 2 & 30   & 2$^{*}$ \\
1.2 & 1 & 1 & 2 & 127 & 2$^{*}$$^{\dagger}$ \\
1.3 & 1 & 1 & 2 & 200 & 2$^{\dagger}$ \\
1.4 & 1 & 1 & 1.9977  & n/a & test failed \\
\hline
0.7 & 2 & 2 & 2 & 13   & 2 \\
0.9 & 2 & 2 & 2 & 12   & 2 \\
1    & 2 & 2 & 2 & 12   & 2 \\
1.2 & 2 & 2 & 2 & 29   & 2 \\
\hline
1    & 3 & 3 & 2 & 8     & 2 \\
1    & 4 & 4 & 2 & 6     & 2 \\
1    & 5 & 5 & 2 & 4     & 2$^{*}$ \\
1.2 & 5 & 5 & 2 & 14   & 2 \\
\hline
1    & 4 & 1 & 2 & 12   & 2 \\
1    & 1 & 4 & 2 & 12   & 2 \\
1    & 4 & 2 & 2 & 8     & 2 \\
1    & 2 & 4 & 2 & 8     & 2 \\
1    & 4 & 3 & 2 & 6     & 2 \\
1    & 3 & 4 & 2 & 6     & 2 \\
\hline 
\end{tabular}\\
$^{*}$used a tighter comparison tolerance of $1 \times 10^{-5}$ and still passed\\
$^{\dagger}$at least one eigenvector iteration did not converge
\end{center}
\label{table:RQIUnitTestRefl}
\end{table}
%-------------------------------------------------------------------------------------------------------

* parameter variation with rqi reflecting + shifted operator, 4 groups (RQI\_UnitTest\_refl)
This is the small RQI unit test with reflecting boundary conditions. This test also used the shifted version of the operator in the preconditioner. Many of these tests did not pass with the preconditioner. The flux was tested against the same loose tolerance unless otherwise noted. The results are show in Table~\ref{table:RQIUnitTestReflShifted}. The max number of Krylov iterations was set to 100 here as well.
%
\begin{table}[!h]
\caption{RQI Unit Test With Reflecting Boundaries and Shifted Operator, Preconditioning Parameter Study}
\begin{center}
\begin{tabular}{c c c l c c}
\hline
Weight & Relaxations & V-cycles & $k$ & Krylov & RQI \\[0.5ex]
\hline
0    & 0 & 0 & 2 & 35 & 2 \\
1    & 1 & 1 & test failed \\
1.4 & 1 & 1 & test failed \\
1    & 2 & 2 & test failed \\
1.4 & 2 & 2 & test failed \\
1    & 3 & 3 & test failed \\
1.4 & 3 & 3 & test failed \\
1    & 4 & 1 & test failed \\
1    & 4 & 2 & test failed \\
1    & 4 & 3 & test failed \\
1    & 4 & 4 & 2 & 18 & 6 \\ 
1    & 3 & 4 & test failed \\
1.2 & 3 & 4 & 2.0024 & 41 & 6 \\
1.3 & 3 & 4 & 1.9997 & 1200 & 12 \\
1.4 & 3 & 4 & 2.0037 & 4100 & 41 \\
1    & 5 & 5 & 2 & 8 & 4$^{*}$ \\
1.2 & 5 & 5 & 2& 32 & 4 \\
\hline 
\end{tabular}\\
$^{*}$used a tighter comparison tolerance of $1 \times 10^{-5}$ and still passed
\end{center}
\label{table:RQIUnitTestReflShifted}
\end{table}
%-------------------------------------------------------------------------------------------------------

  *** conclusions about using the shifted operator ***
 
* parameter variation with rqi reflecting, 27 groups (im\_pi)
This is the infinite medium problem from Chapter~\ref{sec:Chp3} that has 27 groups and a very small dominance ratio. RQI was originally unable to converge the eigenvector for this problem, but got a close guess for the eigenvalue. These results are shown in Table~\ref{table:impi RQI}. This was solved in serial using the debug version of Denovo on a macbook pro. A strange issue with this calculation is that most of the time the flux was correct in magnitude, but negative. Note that the timing comparison should not be taken strictly because other work was being done on the computer while these calculations were running. 
%
\begin{table}[!h]
\caption{Intermediate Problem With Rayleigh Quotient Iteration, Preconditioning Parameter Scoping}
\begin{center}
\begin{tabular}{c c c l l c c}
\hline
Weight & Relaxations & V-cycles & $k$ & Krylov & RQI & Time (s) \\[0.5ex]
\hline
0    & 0 & 0 & 0.397 & 39,025 & 40$^{*}$  & 5.43 $\times 10^{4}$ \\
1    & 1 & 1 & 0.398 & 3,014$^{\dagger}$ & 4 & 2.66 $\times 10^{4}$ \\
1    & 3 & 1 & 0.398 & 50                          & 3 & 1.03 $\times 10^{4}$ \\
1    & 4 & 1 & 0.398 & 44$^{\dagger}$      & 3 & 1.16 $\times 10^{3}$ \\
1    & 2 & 2 & 0.398 & 44$^{\dagger}$      & 3 & 2.86 $\times 10^{2}$ \\
1    & 4 & 4 & 0.4     & 16                          & 3 & 1.99 $\times 10^{3}$ \\
1    & 5 & 4 & 0.4     & 14$^{\dagger}$     & 3 & 2.27 $\times 10^{2}$ \\
1.4 & 5 & 4 & -0.461 & 12$^{\dagger}$    & 2 & 1.74 $\times 10^{3}$ \\
1    & 5 & 5 & -0.457 & 7$^{\dagger}$      & 2 & 1.58 $\times 10^{3}$ \\
\hline
1.1 & 4 & 1 & 0.398 & 43$^{\dagger}$      & 3 & 1.16 $\times 10^{3}$ \\
1.1 & 1 & 4 & 0.398 & 43$^{\dagger}$      & 3 & 1.35 $\times 10^{3}$ \\
1.1 & 2 & 2 & 0.398 & 43$^{\dagger}$      & 3 & 1.18 $\times 10^{3}$ \\
\hline 
\end{tabular}\\
$^{*}$terminated manually\\
$^{\dagger}$negative flux with correct magnitude
\end{center}
\label{table:impi RQI}
\end{table}

An important comparison is that is was solved with power iteration using no preconditioning and using a lot of preconditioning. Without preconditioning the problem took 180 Krylov iterations and $2.57 \times 10^{2}$ seconds. With w1r5v5 preconditioning it took 12 Krylov iterations and $2.19 \times 10^{3}$ sections. Both calculations obtained an eigenvalue of 0.4 and a positive correct flux. 
%-------------------------------------------------------------------------------------------------------

* parameter variation with rqi reflecting using BiCGSTAB, 27 groups (im\_pi)
This problem was attempted with BiCGSTAB instead of GMRES with no success. The preconditioning was set to 2 relaxations per level and 2 V-cycles, the weight was increased in 0.1 increments from 1.0 to 1.5. In the cases where the calculation terminated itself it was because a negative eigenvalue was found (which automatically ends the calculation), and the answers were entirely incorrect. In the three cases where it didn't; w1.1, w1.2, w1.3; the problems were terminated manually. The eigenvalue was oscillating between a value close to the correct answer and a large number like 11. When a problem is terminated manually there is no way to obtain the flux, so it is unknown whether that was correct or not. 

* parameter variation with rqi reflecting, 7 groups (2D Orhtanc)
This used the same settings as the power iteration version. In all cases, except the unpreconditioned one, $k$ was within the uncertainty of the benchmark value. This test case was the first to really focus on whether the preconditioner could get RQI to converge. Recall that this was the first test problem for which RQI did not get the right answer at all. The results of this study can be seen in Table~\ref{table:2D c5g7 rqi}.
%
\begin{table}[!h]
\caption{2D C5G7 Benchmark With Rayleigh Quotient Iteration, Convergence Study}
\begin{center}
\begin{tabular}{c c c l c c c}
\hline
Weight & Relaxations & V-cycles & Krylov & RQI & $<$ 1,000? & Time (s) \\[0.5ex]
\hline
0    & 0 & 0 & 119,006 & 120$^{*}$ & no &9.38 $\times 10^{4}$ \\
1    & 1 & 1 & 16,007   & 17            & no & 2.02 $\times 10^{5}$ \\
1.2 & 1 & 1 & 40,008   & 41$^{*}$   & no & 2.06 $\times 10^{5}$ \\
1    & 3 & 1 & n/a         & n/a$^{*}$  & 7   & n/a \\
1    & 2 & 2 & 11,158   & 19            & alternated & 3.99 $\times 10^{5}$ \\
1    & 3 & 2 & 3,320     & 19            & 14 & 1.64 $\times 10^{5}$ \\
\hline
1    & 3 & 3 & 299        & 19            & yes & 2.57 $\times 10^{4}$ \\
1.1 & 3 & 3 & 281        & 19            & yes & 2.40 $\times 10^{4}$ \\
1.3 & 3 & 3 & 254        & 19            & yes & 2.19 $\times 10^{4}$ \\
1.5 & 3 & 3 & n/a         & n/a$^{*}$ & no & n/a \\
\hline 
\end{tabular} \\
$^{*}$terminated manually
\end{center}
\label{table:2D c5g7 rqi}
\end{table}

The ``$<$ 1,000?'' column indicates whether or not the multigroup iterations converged during the RQI process. If the value is ``no'' that means the eigenvector only converged during the first iteration when the Rayleigh quotient was not yet a good guess and the system was therefore not yet ill-conditioned. A number indicates the last eigenvalue iteration for which the Krylov method took less than 1,000 iterations. A ``yes'' means all of the Krylov iterations converged.

These results show a few important things. The first is another example that preconditioning can hold RQI on track enough to get the right eigenvalue, even when the eigenvector is not quite converging. For many of the calculations the eigenvector didn't converge, or didn't converge all the time, but the correct eigenvalue was still found (even in the w1.2r1v1 case). As the preconditioning increased, the eigenvector came closer to converging for all iterations. 

The second is that with enough preconditioning, the eigenvector within RQI can be converged and the right eigenpair can be found. For all of the w\#r3v3 cases all of the Krylov iterations converged. As a result, the calculation time decreased by an order of magnitude. Increasing the weight then decreased iteration count and wall time up until there was too much weight and the calculation did not converge at all. 
%-------------------------------------------------------------------------------------------------------

* parameter demonstration with rqi reflecting using BiCGSTAB, 7 groups (2D Orthanc)
 
* parameter variation with rqi reflecting, 7 groups (3D Orhtanc)
The 3D benchmark was also done using the same parameters as the power iteration version. The degree to which the RQI-computed $k$ matched the benchmark was the same as PI: within 0.011 when the tighter calculation tolerance was used, but not within the reported uncertainty. The results are in Table~\ref{table:3D c5g7 rqi}.
%
\begin{table}[!h]
\caption{3D C5G7 Benchmark With Rayleigh Quotient Iteration, Preconditioning Parameter Scoping}
\begin{center}
\begin{tabular}{c c c c l l}
\hline
Weight & Relaxations & V-cycles & Krylov & PI & Time (s) \\[0.5ex]
\hline
0    & 0 & 0 & n/a     & n/a          & exceeded wall time \\
1    & 1 & 1 & n/a     & n/a          & exceeded wall time \\
1.5 & 1 & 1 & n/a     & n/a          & exceeded wall time \\
1.2 & 2 & 1 & n/a     & n/a          & exceeded wall time \\
1.3 & 2 & 2 & 302    & 19           & 2.32 $\times 10^{4}$ \\
1    & 3 & 3 & 103    & 9$^{*}$    & 3.02 $\times 10^{4}$ \\
1    & 3 & 3 & 164    & 15$^{\dagger}$ & 3.38 $\times 10^{4}$ \\
1.5 & 3 & 3 & 187    & 19           & 3.24 $\times 10^{4}$ \\
1    & 4 & 4 & n/a     & n/a          & exceeded wall time \\
1    & 4 & 4 & 74     & 9$^{*}$    & 2.29 $\times 10^{4}$ \\
1.5 & 5 & 5 & n/a     & n/a          & exceeded wall time \\
\hline 
\end{tabular}\\
$^{*}$tol and upscatter tol = 1 $\times 10^{-5}$, $k$ tol = 1 $\times 10^{-3}$\\
$^{\dagger}$tol and upscatter tol = 1 $\times 10^{-4}$, $k$ tol = 5 $\times 10^{-5}$
\end{center}
\label{table:3D c5g7 rqi}
\end{table}  

What is likely happening when the problems with less preconditioning run out of time is that the eigenvector is not converging. As was seen before, the calculations take a long time when ever eigenvalue iteration uses 1,000 Krylov iterations. It may be that the eigenvalue is correct or nearly correct, but this machine does not store any output from jobs that exceed the wall time limit. Thus, no information about what is happening during the calculation can be gleaned. 

When the problem does finish and return results in time, it does so in fewer eigenvalue iterations, fewer Krylov iterations, and less time than Power Iteration. It even manages to finish in time for a calculation when PI did not. 
%-------------------------------------------------------------------------------------------------------
  
  *** conclusions about precond making rqi possible ***

*** conclusions about parameters based on all experiences ***

%-------------------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------
\subsubsection{Multisets}
Shows that preconditioner actually works great with multisets and we probably chose the right implementation.

* multisets with fixed source, 27 groups (FeC\_vac)
The last but certainly not least area of investigation was how the preconditioner faired when using multisets. To investigate its effect on the Krylov iterations without worrying about impacts of an eigenvalue calculation, the iron graphite fixed source problem was investigated. This was run on orhtanc using the optimized version of the code. 

To make the problem large enough to be able to use energy sets, the grid was increased to $50 \times 50 \times 50$, though $S_{4}$ was used instead of $S_{8}$. The unpreconditioned version was compared to one with w1r2v2 on 1 to 10 sets. Note that with 27 groups 10 sets is the maximum possible to still be able to use the preconditioner. In addition, 2 x-blocks, 2 y-blocks, and 1 z-block were used. The calculations therefore used between 4 and 40 cores. With all set combinations the preconditioned calculation took 27 GMRES iterations while the unpreconditioned took 123.

Because the number of iterations did not change with sets, the only thing to compare is time. The focus here is on relative change in time rather than absolute time since there is still room for the preconditioner to be optimized. A Table with the data can be found in Appendix~\ref{sec:AppendixD}, Table~\ref{table:FeC multisets}.

Three plots are shown in Figure~\ref{fig:FeC multisets}. From the top the first shows the wall time for the preconditioned and unpreconditioned (regular) calculations as a function of time. The second plots the ratio of the time the calculation with N sets took to the time with 1 set. The last plot is of the relative difference between the two times.
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.7\textwidth, height=0.8\textheight] {FeCmultisets}
   \end{center}
   \caption{Multiset Study with Preconditioner for Iron Graphite Fixed Source Problem}
   \label{fig:FeC multisets}
\end{figure}

This test was also run with increased preconditioning parameters for 1 set and 10 sets to see if that made a difference. The angle set was changed back to $S_{8}$ and there was no decomposition in space. When w1.3r4v4 was used the number of Krylov iterations decreased to 11. With 1 set this took 6.49 $\times 10^{4}$ sec to complete. With 10 sets this was reduced to 5.02 $\times 10^{3}$ sec. A 10 fold increase in computing power gave more than a 10 fold decrease in run time. 

When the problem was not preconditioned 124 Krylov iterations were needed. With 1 set the wall time was 8.81 $\times 10^{3}$ sec and with 10 sets it was 1.04 $\times 10^{3}$ sec. Without preconditioning a 10 fold increase in computing power gave less than a 10 fold decrease in run time.
%-------------------------------------------------------------------------------------------------------

* multisets with power iteration, 27 groups (im\_pi)
The infinite medium, 27 group problem was also used to study how the preconditioner faired with energy set decomposition. Because this problem has reflecting boundary conditions it could only be used with power iteration and not RQI (recall, RQI cannot do multisets with reflecting boundaries at this time). This problem was also run on orthanc using between 1 and 10 sets. No other problem parameters were changed from what was presented above. 

The preconditioning parameters were w1r2v2 which brought the Krylov iterations from 180 (90 Krylov per PI with 2 PI) to 46 (23 Krylov per PI with 2 PI). Three plots are shown in Figure~\ref{fig:impi multisets}, they are of the same variables as in the iron cube problem. A table of the data can be found in Appendix~\ref{sec:AppendixD}, Table~\ref{table:impi multisets}.
%
\begin{figure}[!ht]
    \begin{center}
      \includegraphics [width=0.7\textwidth, height=0.8\textheight] {impimultisets}
   \end{center}
   \caption{Multiset Study with Preconditioner for Infinite Medium Eigenvalue Problem}
   \label{fig:impi multisets}
\end{figure}

This test was run with increased preconditioning parameters for 1 set and 10 sets as well. When w1r4v4 was used the number of Krylov iterations decreased to 8 per eigenvalue iteration, and 2 eigenvalue iterations were needed. With 1 set this took 4.02 $\times 10^{2}$ sec to complete. With 10 sets this was reduced to 2.94 $\times 10^{1}$ sec. For this problem, too, a 10 fold increase in computing power gave more than a 10 fold decrease in run time. 

When the problem was not preconditioned with 1 set the wall time was 56.9 sec and with 10 sets it was 7.32 sec. Once again, without preconditioning a 10 fold increase in computing power gave less than a 10 fold decrease in run time.
 
* multisets with power iteration, 44 groups (Full\_PWR)
 
* multisets with rqi, 44 groups (Full\_PWR) 

*** conclusions about set - groups/set tradeoffs and overall usefulness of precond with multisets ***

1) this indicates that the improvement from the preconditioner does not come from the depth of V-cycle. If only going down 1 or 2 grids has as much of an impact as going down 6 perhaps it is not always necessary to coarsen down to one group. However, this is only one problem. 

2) preconditioning has a super-linear improvement with energy sets. This is because it keeps the number of iterations the same, but each application of the preconditioner becomes less costly since the V-cycle becomes shallower. Once it is optimized this may become less super-linear since it will take a smaller total fraction of the runtime. Nevertheless, this is a good property. 

%-------------------------------------------------------------------------------------------------------
\subsection{Implications}

%The goal for the new preconditioners is to decrease solution time for at least some categories of problems. There are a variety of problem types which have been found to be challenging for transport methods. Traditional iterative schemes such as power iteration and Gauss Seidel have a hard time converging systems with highly scattering materials such as graphite because of their spectral radii. As noted in \cite{Rosa2010}, restarted Krylov methods can stagnate when there are optically thin spatial regions. Trying one toy problem from each of these categories is likely to be instructive about the success of the new methods. A large scale real problem, such as the full PWR used in the previous scaling studies, should also be tried. 


%This preconditioning idea is new for the neutron transport equation. The author was unable to find a similar idea in the literature. This is not surprising because the author was also unable to find deterministic transport solvers in which the energy groups have been decoupled. The decoupling that is facilitated by the MG Krylov solver is what enables the use of this preconditioner.
