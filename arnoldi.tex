% Prelim, Appendix B
% by Rachel Slaybaugh

\chapter{Krylov Methods}
\label{sec:AppendixB}
This appendix is intended to provide the mathematical details of Krylov methods, including information about how restarting Arnoldi and GMRES. While understanding this is not crucial to the developed methods, it is beneficial to be aware of how the underlying solvers work. 

\section{Arnoldi Method}
A general issue with Krylov subspaces is that the columns of $\mathcal{K}_{k}(\ve{A},v_{1})$ become increasingly linearly dependent with increasing $k$. Trying to build solutions out of linear combinations of nearly linearly dependent vectors can fail. The columns of the subspace can be orthogonalized to solve this problem. However, direct orthogonalization will cause some information contained in the subspace to be lost, making more sophisticated factorization methods useful \cite{Stewart2001}. 

In general there are two factorization methods upon which many Krylov methods are based. Some use the Arnoldi method, which generates an orthonormal basis for the Krylov subspace for non-normal matrices, and others use the Lanczos method which creates non-orthogonal bases for normal matrices \cite{Knoll2004}, \cite{Stewart2001}. Denovo has the option of using either restarted GMRES or BiCGSTAB, both of which use the Arnoldi method \cite{Evans2009}.  The remainder of this subsection will be devoted to developing an understanding of the Arnoldi method.

Fundamentally, Krylov methods are Galerkin or Galerkin-Petrov methods on a Krylov subspace. Galerkin's method uses a few fundamental concepts. One is that an inner product of two functions is zero when the functions are orthogonal: $<f(x), g(x)> = 0$ if $f(x)$ and $g(x)$ are orthogonal. Another is that any function, $f(x)$, in a subspace, $\mathcal{V}$, can be written as a linear combination of the vectors that make a basis for that function space. Let $\ve{V} = \{\phi_{i}(x) \}_{i=0}^{\infty}$ be the basis for $\mathcal{V}$; if $f(x) \in \mathcal{V}$ then $f(x) = \sum_{j=0}^{\infty} c_{j} \phi_{j}(x)$ for some scalar coefficients $c_{j}$ \cite{Matthews2005}.  

A weighted residual method is a solution technique for solving some linear problem $\ve{A}u = b$ where $u = u_0 + \sum_{i=1}^{n} c_i \phi_i(x)$ is the approximate solution and $u_{0}$ is an initial guess. The solution is found by taking the inner product of some arbitrary weight function, $w(x)$, and the residual, $r(x) = b - \ve{A}u$, $r(x) \in \mathcal{V}$, such that $<w(x), r(x)> = 0$. The solution is the $u$ satisfying this requirement. Galerkin's method is a weighted residual method where the weight function is chosen from the basis functions: $w(x)$ is selected from $\ve{V}$ \cite{Matthews2005}. In the Galerkin-Petrov method, the weight functions come from a subspace other than $\mathcal{V}$, that is $w(x) \in \mathcal{W}$ \cite{Skeel2006}. 

The weighted residual can also be thought of as a process to minimize the residual. There are a few ways to express this idea. If $\hat{u}$ is the exact minimizer of $r(x) = b - \ve{A}\hat{u}$, let $u' = \hat{u} + w(x)$ be a close approximation to $\hat{u}$ with $w(x) \in \mathcal{V}$. The residual is minimized if and only if $<w(x), r(x)> = 0$ for all $w(x) \in \mathcal{V}$, meeting the Galerkin condition just described. This can also be written as: 
%
\begin{align}
  \text{find } \qquad u \in u_{0} + \mathcal{V} \qquad \text{such that} \qquad r(x) \perp \mathcal{V} \:.
  \label{eq:galerkinMin}
\end{align}
%
Further, if $y$ is the solution to
\begin{equation}
  \ve{V}^{T}\ve{AV}y = \ve{V}^{T} r_{0} \:, \qquad \text{then} \qquad \hat{u} = u_{0} + \ve{V}y \:.
  \label{eq:minResidual}
\end{equation}
This $\hat{u}$ minimizes the residual in the some measure of interest, where the measure is determined by the selection of $y$. In the Petrov-Galerkin formulation, $\ve{W}$ is a basis for subspace $\mathcal{W}$ and the idea is to find $ u \in u_{0} + \mathcal{V}$ such that $r(x) \perp \mathcal{W}$. Now $\ve{V}^T\ve{AW}y = \ve{V}^T r_0$ \cite{Skeel2006}.

Sorensen~\cite{Sorensen1996} and Stewart~\cite{Stewart 2001} use the Galerkin Method in combination with Ritz pairs to understand the Arnoldi method, and this viewpoint will be developed here. To maintain consistency with the main portion of this work, the subspace from which solutions are derived in the next paragraphs will be the Krylov subspace $\mathcal{K}_{k}(\ve{A},v_{1})$, and the equation will be switched to $\ve{A}x = b$.

A vector $z \in \mathcal{K}_{k}(\ve{A},v_{1})$ is defined as a Ritz vector with corresponding Ritz value, $\theta$, if it satisfies the Galerkin condition $<w, \ve{A}z - \theta z> = 0 $ for all $w \in \mathcal{K}_{k}(\ve{A}, v_{1})$. To see why Ritz pairs can be important, define $\hat{p}(\ve{A})$ to be the minimum characteristic polynomial of $\ve{A}$ such that $||\hat{p}(\ve{A})v_{1}|| \le ||q(\ve{A})v_{1}||$ for all monic polynomials $q \ne \hat{p}$ of degree $k$. If $(\theta, z)$ is a Ritz pair for $\ve{A}$, then $\ve{A}z - z\theta = \gamma \hat{p}(\ve{A})v_{1} = g$ for some scalar $\gamma$. When $g = 0$, then the Ritz pair is an eigenpair. When $g$ is small, the Ritz pair is likely a close approximation to an eigenpair of $\ve{A}$ \cite{Sorensen1996}. 

These ideas can be assembled to understand why the Arnoldi method works. Let $\ve{V}$ be a basis for a Krylov subspace $\mathcal{K}_{k}(\ve{A},v_{1})$, and $\mathcal{X} \in \mathcal{K}_{k}(\ve{A},v_{1})$ be an eigenspace of $\ve{A}$. When solving $\ve{A}x = b$, the subspace $\mathcal{K}_{k}(\ve{A},v_{1})$ and the vector $x$ must satisfy
\begin{enumerate}
  \item $x \in \mathcal{K}_{k}(\ve{A},v_{1})$ and
  \item $r = \ve{A}x - b \perp \mathcal{K}_{k}(\ve{A},v_{1})$. 
\end{enumerate} 
%
Let $\ve{H} = \ve{V}^{T}\ve{AV}$. There is an eigenpair $(\theta, x)$ of $\ve{H}$ such that $(\theta, \ve{V}x)$ is an eigenpair of $\ve{A}$. To reduce notational clutter let $z = \ve{V}x$, giving $\ve{A}z = \theta z$ \cite{Stewart2001}. 

If $\ve{V}$ is an orthonormal basis for  $\mathcal{K}_{k}(A,v_{1})$, then $(\theta, z)$ is a Ritz pair if and only if $x = \ve{V}y$ with $\ve{H}y = \theta y$ for some $y$. Noting the definition of $\ve{H}$, comparing to Equation \eqref{eq:minResidual}, and doing some basic matrix manipulation, it is clear that this $y$ minimizes the residual. As the residual tends toward zero, the Ritz pair converges to an approximate eigenpair of $\ve{A}$ \cite{Stewart2001}. Another way to state this is to revisit the polynomial identity expressing the minimum residual. It can be shown that $\ve{AV}y - \ve{VH}y = \gamma \hat{p}(\ve{A})v_{1} = g$. As $g \to 0$, the Ritz pair approaches the eigenpair of $\ve{A}$ \cite{Sorensen1996}. 

In summary, the eigenpairs of $\ve{A}$ are approximated by the eigenpairs of $\ve{H}$. These eigenvalues and/or eigenvectors are subsequently used in different ways by different Krylov methods to formulate the solution to $\ve{A}x = b$ to achieve specific goals, like minimizing the residual in a certain norm. The Galerkin condition is used to ensure the eigenpairs of $\ve{H}$ become increasingly good approximations to those of $\ve{A}$ as the size of the Krylov subspace increases. 

The Arnoldi method is a process of establishing the $\ve{V}$ and $\ve{H}$ discussed above \cite{Stewart2001}. $\ve{H}$ is an orthogonal projection of $\ve{A}$ onto the basis $\ve{V}$ and is upper Hessenberg in form. The Gram-Schmidt method (or the modified Gram-Schmidt method) computes $\ve{V}$. The Arnoldi method generates a Ritz estimate for the Ritz pair at each iteration. Using these terms and ideas, the Arnoldi algorithm shown in Algorithm~\ref{algo:Arnoldi} can be understood \cite{Saad1986}, \cite{Sorensen1996}:
%
\begin{algorithm}
  \caption{ Orthogonal Arnoldi Method}
  $r_0 = b - \ve{A}x_0$ \\
  $v_1 = \frac{r_0}{||r_0||}$ \\
  For $j = 1$ to $k$: 
  \begin{list}{}{\hspace{2em}}
    \item $h_{i,j} = v_i \ve{A} v_j$, $i = 1, \dots, j$ 
    \item $\text{ } \hat{v}_{j+1} = \ve{A}v_j - \sum_{i=1}^{j} v_j h_{i,j}$ 
    \item $\text{ } h_{j+1,j} = ||\hat{v}_{j+1}||$ 
    \item $\text{ } v_{j+1} = \frac{\hat{v}_{j+1}} {h_{j+1,j}}$ 
  \end{list}
  End For \\
  Form the solution $x_k = x_0 + \ve{V}_k y_k$, where $y_k = \ve{H}_k^{-1} ||r_0|| e_1$
  \label{algo:Arnoldi}
\end{algorithm}

The $k$th step of an Arnoldi factorization can be written as: 
%
\begin{equation}
  \ve{AV}_{k} = \ve{V}_{k}\ve{H}_{k} + g_{k}e_{k}^{T} \:,
\end{equation} 
%
where $e_{k}$ is the $k^{th}$ column of the identity matrix and $g$ is the residual. An alternate way to derive the Arnoldi method is as a truncation of the reduction of $\ve{A}$ to Hessenberg form using shifted QR-iteration \cite{Sorensen1996}.

\section{Restarting Arnoldi}
As mentioned above, Krylov methods can become untenable if the subspace is allowed to become large. Restart methods have been developed to handle this. After $m$ iterations the Arnoldi process is halted and a new starting vector, $\hat{v}_1$, is constructed from information gained during the previous $m$ iterations. This new vector is used as the starting vector for another round of the Arnoldi process, which is now using the subspace $\mathcal{K}_m(\ve{A},\hat{v}_1)$ rather than $\mathcal{K}_m(\ve{A},v_1)$. This process is repeated, finding new a starting vector every time the subspace reaches dimension $m$, until convergence. It is assumed that of the $m$ eigenvalues available, only $r$ of them are wanted. Two such methods, explicit and implicit, will be discussed here from the viewpoint of how they work and how they preserve the information of interest. 

Explicit restarting applies a polynomial to the starting vector that is designed to damp unwanted components of the eigenvector expansion. There are a few different forms of explicit restarting, but the basic idea begins with splitting the Ritz values into a wanted set, $\Omega_w$ and an unwanted set $\Omega_u$. In one form of explicit restarting, a combination of wanted eigenvalues are used and in another form a combination of unwanted eigenvectors are used. In both formulations a filter polynomial is used to create a $\hat{v}_1$ that minimizes the residual $g$ by removing the unwanted information. The components of the new starting vector therefore become more aligned toward the directions of interest. Unfortunately, filter polynomials are expensive to apply directly and this motivates the need for a more sophisticated restart method \cite{Sorensen1996}, \cite{Stewart2001}.

Implicit restarting is designed to be less expensive than explicit restarting. It uses an implicitly shifted QR mechanism to compress the factorization while retaining information of interest. The idea is to go from a factorization of length $m = p + r$ to one of length $r$ by applying $p$ shifts implicitly. For $j = 1,2,\dots,p$, a QR factorization is done with shifts $\mu_j$ where the shifts are derived from the spectrum of $\ve{H}$, $\sigma(\ve{H}) \equiv \{\lambda \in \mathbb{R}: rank(\ve{A} - \lambda \ve{I}) < n\}$. This gives an $r$-step Arnoldi factorization of the form $\ve{AV}_r \ve{Q} = (\ve{V}_r \ve{Q})(\ve{Q}^T \ve{H}_r \ve{Q}) + g_k e^T_k$. $\ve{Q} = \ve{Q}_1 \ve{Q}_2 \dots \ve{Q}_p$ where $\ve{Q}_j$ is an orthogonal matrix corresponding to shift $\mu_j$, and $g_r = (\ve{V}_r \ve{Q}) e_{r+1} \beta_r + g_m \sigma(\ve{H})$ \cite{Sorensen1996}.

The shift selection strategy can be designed to create a minimal $\hat{v}_1$. The shifted QR factorization reduces the subspace size while eliminating the undesired components. In essence this is implicitly applying a filter polynomial while avoiding matrix-vector multiplications that would be required in the explicit method. Further, $\mathcal{K}_p(\ve{A},v_1) \subset \mathcal{K}_m(\ve{A}, \hat{v}_1)$, which means the wanted Ritz vectors are still represented in the updated subspace. The implicit method is more efficient and more numerically stable than the explicit restart method \cite{Sorensen1996}, \cite{Stewart2001}.

\section{GMRES}
One of the more popular Krylov methods, which uses the Arnoldi process, is the GMRES algorithm developed by Saad and Schultz~\cite{Saad1986}, \cite{Knoll2004}. Recall that $\ve{V}_{k}$ is an orthonormal basis for the Krylov subspace $\mathcal{K}_{k}(\ve{A}, v_1)$ and that $\ve{H}_{k}$ is the representation of the part of $\ve{A}$ that is in this Krylov subspace formed from the basis $\ve{V}_{k}$. The notation $\bar{\ve{H}}^{k}$ is the $(k+1) \times k$ upper Hessenberg matrix that includes the newest $h_{k+1,k}$ element generated in the Arnolodi process \cite{Saad1986}. 

The distinguishing factor for different Krylov methods is the way in which they select the $y_k$ that makes the solution $x_k = x_0 + \ve{V}_k y_k$. Let $z = \ve{V}_k y_k$. GMRES uses the least squares procedure to find the $y_k$ that minimize the norm of the residual over $z$ in $\mathcal{K}_{k}(\ve{A}, v_1)$. To accomplish this, the least squares problem
%
\begin{equation}
  \min_{z \in \mathcal{K}_{k}} ||r_{0} - \ve{A}z||
  \label{eq:least-squares}
\end{equation} 
%
is solved, giving the solution $z = \ve{V}_{k} y_{k}$. The $y_{k}$ that is selected minimizes $||\beta e_{1} - \bar{\ve{H}}^{k} y||$, where $\beta = ||r_{0}||$  \cite{Saad1986}. 

GMRES picks the best solution within the Krylov subspace with respect to minimizing the residual \cite{Ipsen1998}. %The minimization is done by progressively applying plane rotations in such a way that the residual norm can always be easily and efficiently obtained to analyze convergence. 
This method cannot break down unless it has already converged, in which case the answer has been found. GMRES has all of the storage issues associated with Krylov methods mentioned before, but restarted GMRES can be used to mitigate this concern \cite{Saad1986}.


